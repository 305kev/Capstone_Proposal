{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "import re\n",
    "from time import sleep\n",
    "from urlparse import urljoin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Look at the Data\n",
    "\n",
    "# print soup.prettify() \n",
    "\n",
    "# print soup.title\n",
    "\n",
    "# print soup.table.find_all('td')\n",
    "\n",
    "# print soup.td.string\n",
    "\n",
    "# jumble.find_all('td.string')\n",
    "\n",
    "## print soup.find_all('td')\n",
    "\n",
    "\n",
    "# rows = soup.select('table.mod-data tbody tr')\n",
    "# print soup.find_all('td')\n",
    "\n",
    "# print soup.prettify() \n",
    "\n",
    "\n",
    "# for i in soup.find_all(name=\"span\"): \n",
    "#     print i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-1st-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'\n",
    "\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "\n",
    "def get_page_urls(soup): \n",
    "    \"\"\"\n",
    "    Get all the search result, case urls from a single page.\n",
    "    :param soup: Beautiful soup object\n",
    "    :return: url_list, the desired urls to be checked\n",
    "    \"\"\"\n",
    "    url_list = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        if 'circuit' in link.get('href'): \n",
    "            url_list.append(link.get('href'))\n",
    "    return url_list\n",
    "\n",
    "# url_list = []\n",
    "# for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "#     if 'circuit' in link.get('href'): \n",
    "#         url_list.append(link.get('href'))\n",
    "\n",
    "# for i in (url_list): \n",
    "#     print i\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Working Urls Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function for checkin every site \n",
    "\n",
    "# url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-1st-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'\n",
    "\n",
    "# content = requests.get(url).content\n",
    "# soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "url_list = []\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    if 'circuit' in link.get('href'): \n",
    "        url_list.append(link.get('href'))\n",
    "        \n",
    "def get_next_url(current_url, soup):\n",
    "    \"\"\"\n",
    "    Get the URL of the next listings page.\n",
    "    :param soup: Beautiful soup object\n",
    "    :return: string, the URL of the next webpage\n",
    "    \"\"\"\n",
    "    \n",
    "    d = [ i.get('href') for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}) ][-1]\n",
    "    return urljoin(current_url, d)\n",
    "\n",
    "        \n",
    "        \n",
    "def create_soup(url):\n",
    "    \"\"\"\n",
    "    Get the HTML contents of the URL.\n",
    "    :param url: string, the url to scrape\n",
    "    :return: soup: a BeautifulSoup object\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    return BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def second_check(url):\n",
    "    \n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    for i in soup.find_all(\"p\"): \n",
    "#         print i, type(i)\n",
    "        try: \n",
    "            href= i.a.get('href')\n",
    "            return  href\n",
    "            break\n",
    "    \n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "def first_check(url_list): \n",
    "    ## Get case text from second layer link if in initial check, otherwise check a second time\n",
    "    ## for an additional internal link in second check.\n",
    "    ## Return confirmed links with case texts: \n",
    "\n",
    "    target_urls = []\n",
    "    \n",
    "    for site in url_list:\n",
    "        new_soup = create_soup(site)\n",
    "\n",
    "        for html in new_soup.find_all(name=\"div\", attrs={\"class\": \"btn_read\"}): \n",
    "            check_url= html.a.get('href')        \n",
    "            check_soup = create_soup(check_url)\n",
    "            \n",
    "            if check_soup.find_all(\"a\", { \"target\" : \"new\"}): \n",
    "#                 print \"NEEDED A SECOND CHECK\"\n",
    "                target_urls.append(second_check(check_url))\n",
    "#                 print \"BUT ADDED: \", second_check(check_url), \"On the Second Check\"\n",
    "\n",
    "            else: \n",
    "                target_urls.append(check_url)\n",
    "#                 print 'ON THE FIRST TRY, ADDED: ', check_url\n",
    "        sleep(2)\n",
    "    return target_urls\n",
    "\n",
    "# first_check(url_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Check Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-1st-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}) \n",
    "\n",
    "\n",
    "print   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_scraper(current_url, dft):\n",
    "    \"\"\"\n",
    "    Run the web scraper that will scrape Findlaw\n",
    "    :param current_url: string, the initial URL to scrape\n",
    "    :param dft: Pandas dataframe, the listings data from previous scrapings\n",
    "    :return: dft: Pandas dataframe containing scraped information\n",
    "    \"\"\"\n",
    "#     content = requests.get(url).content\n",
    "#     soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "\n",
    "#     for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "#         next_list.append(i.text)\n",
    "    \n",
    "    flag = u'\\xbb'\n",
    "    list_of_urls = []\n",
    "    soup = create_soup(current_url)\n",
    "\n",
    "    # Run the scraper until it runs out of pages to scrape\n",
    "    run = True\n",
    "    \n",
    "    while u'\\xbb' in flag and run==True:\n",
    "        ## \"Keep Going; there was an error trying to make >> a string\"\n",
    "        ## (1) From soup fill next_list with page links at the bottom + next symbol \n",
    "        next_list =[]\n",
    "\n",
    "        for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "            next_list.append(i.text)\n",
    "\n",
    "#             flag = next_list[-1]\n",
    "\n",
    "        ## (2) Get URLS using Check Functions \n",
    "        initial_url_list= get_page_urls(soup)\n",
    "        confirmed_urls = first_check(initial_url_list)\n",
    "\n",
    "        with open('output.txt', 'a') as f: \n",
    "            for link in confirmed_urls: \n",
    "                f.write(link + '\\n')\n",
    "\n",
    "#             dft.to_csv('confirmed_urls.txt', sep=' ', index=False, header=False)\n",
    "\n",
    "\n",
    "        ## (3) Get court texts \n",
    "\n",
    "\n",
    "        ## (4) Get next page's pagination info \n",
    "\n",
    "        current_url = get_next_url(current_url, soup)         \n",
    "        soup = create_soup(current_url)\n",
    "\n",
    "        next_list =[]\n",
    "\n",
    "        for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "            next_list.append(i.text)\n",
    "            sleep(1)\n",
    "\n",
    "        flag = next_list[-1]\n",
    "        run = False\n",
    "            \n",
    "#         dft = dft.append(pd.DataFrame(listings), ignore_index=True)\n",
    "        \n",
    "    return \n",
    "\n",
    "\n",
    "# df_new = pd.DataFrame(columns=[\"job_title\", \"location\", \"company\",\n",
    "#                                    \"url\", \"jobsite\", \"job_description\"])\n",
    "\n",
    "df = pd.DataFrame(columns=[\"url\"])\n",
    "first_url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-1st-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'\n",
    "run_scraper(first_url, df)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YESSSSS\n"
     ]
    }
   ],
   "source": [
    "url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-1st-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'\n",
    "\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "\n",
    "# flag = \"\\xbb\"\n",
    "# list_of_urls = []\n",
    "\n",
    "#     # Run the scraper until it runs out of pages to scrape\n",
    "\n",
    "# while \"\\xbb\" in flag:\n",
    "#     soup = create_soup(url)\n",
    "    \n",
    "#     next_list =[]\n",
    "\n",
    "#     for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "#         next_list.append(i.text)\n",
    "#         print \n",
    "\n",
    "#     flag = next_list[-1]\n",
    "\n",
    "\n",
    "next_list =[]\n",
    "\n",
    "# str = unicode(str, errors='replace')\n",
    "\n",
    "\n",
    "for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "    next_list.append(i.text)#.encode(\"utf-8\")))\n",
    "\n",
    "\n",
    "flag= next_list[-1]\n",
    "    \n",
    "# print next_list\n",
    "# print flag , type(flag)\n",
    "\n",
    "# flag = u'4'\n",
    "\n",
    "#     try: \n",
    "#         flag = str(flag)\n",
    "#         print flag, type(flag)\n",
    "#         print 'You can BREAK from the while loop now; converting flag to a STRING was successful\\\n",
    "#      meaning that there are NO MORE next signs'\n",
    "\n",
    "#     except: \n",
    "\n",
    "#         print \"Keep Going; there was an error trying to make >> a string\"\n",
    "\n",
    "if u\"\\xbb\" in flag: \n",
    "    print 'YESSSSS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if \"\\xbb\" in flag: \n",
    "#     print 'YESSSSS'\n",
    "\n",
    "# # next_list= next_list[0:-1]\n",
    "\n",
    "\n",
    "\n",
    "# try: \n",
    "#     type(int(next_list[-1])) != type(int)\n",
    "    \n",
    "#     print \"Yes!!\"\n",
    "    \n",
    "# except: \n",
    "#     print \"No!!\"\n",
    "    \n",
    "\n",
    "# print next_list\n",
    "\n",
    "# while type(int(next_list[-1])) != type(int): \n",
    "#     for i in (range(1,5)):\n",
    "#         next_list.append(i)\n",
    "    \n",
    "#     print next_list\n",
    "#     next_list.append('i')\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "# print soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print soup.find(name=\"span\", attrs={\"class\": \"pgnum\"}).text\n",
    "                \n",
    "# for next_ in soup.find_all(name=\"span\", attrs={\"a\": \"title\"}): \n",
    "#     print next_['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for next_ in soup.find_all(name=\"span\", attrs={\"a\": \"title\"}): \n",
    "    print next_['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-1st-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'\n",
    "\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "next_list =[]\n",
    "  \n",
    "while type(int(next_list[-1])) != type(int): \n",
    "    for i in (range(1,5)):\n",
    "        next_list.append(i)\n",
    "    \n",
    "    next_list.append('i')\n",
    "    \n",
    "print next_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Testing Collections/ Dictionary\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'list'>, {'City': ['dallas', 'austin'], 'Job Info': ['....', 'blah...'], 'Job Title': ['DS', 'data scientist']})\n"
     ]
    }
   ],
   "source": [
    "my_dict = defaultdict(list)\n",
    "\n",
    "ex= [[\"DS\", \"dallas\", \"....\"], ['data scientist', 'austin', 'blah...'] ]\n",
    "for i in ex: \n",
    "    for ind, e in enumerate(i): \n",
    "#         print e\n",
    "        if ind==0: \n",
    "            my_dict['Job Title']+= [e]\n",
    "        elif ind==1: \n",
    "            my_dict['City']+= [e]\n",
    "        else: \n",
    "            my_dict['Job Info']+= [e]\n",
    "        \n",
    "print my_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
