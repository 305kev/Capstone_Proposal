{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyparsing import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', 'F.3d', '1406, 1409']\n",
      "- abbrev: F.3d\n",
      "- pages: 1406, 1409\n",
      "- volume: 12\n",
      "\n",
      "['308', 'N. J. Super.', '516']\n",
      "- abbrev: N. J. Super.\n",
      "- pages: 516\n",
      "- volume: 308\n",
      "\n",
      "['70', 'A. 2d', '270 ']\n",
      "- abbrev: A. 2d\n",
      "- pages: 270 \n",
      "- volume: 70\n",
      "\n",
      "['422', 'U.S.', '490, 499 n. 10']\n",
      "- abbrev: U.S.\n",
      "- pages: 490, 499 n. 10\n",
      "- volume: 422\n",
      "\n",
      "['95', 'S.Ct.', '2197, 2205 n. 10']\n",
      "- abbrev: S.Ct.\n",
      "- pages: 2197, 2205 n. 10\n",
      "- volume: 95\n",
      "\n",
      "['45', 'L.Ed.2d', '343 ']\n",
      "- abbrev: L.Ed.2d\n",
      "- pages: 343 \n",
      "- volume: 45\n",
      "\n",
      "['843', 'F.Supp.', '1526 ']\n",
      "- abbrev: F.Supp.\n",
      "- pages: 1526 \n",
      "- volume: 843\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "raw = \"\"\"Indemnified Capital Investments, S.A. v. R.J. O'Brien & Assoc., Inc., \n",
    "12 F.3d 1406, 1409 (7th Cir.1993). The New Jersey Superior Court's Appellate \n",
    "Division affirmed the dismissal of Dale's common-law claim, but otherwise \n",
    "reversed and remanded for further proceedings, 308 N. J. Super. 516, blah 70 A. 2d 270 (1998). \n",
    "See also Warth v. Seldin, 422 U.S. 490, 499 n. 10, 95 S.Ct. 2197, 2205 n. 10, 45 L.Ed.2d 343 (1975).\n",
    "NFMA, NEPA, or MUSYA. Sierra Club v. Marita, 843 F.Supp. 1526 (E.D.Wis.1994) (\"Nicolet \").\"\"\"\n",
    " \n",
    "# some basic things to match for\n",
    "integer = Word(nums)\n",
    "ordinal = Combine(integer + oneOf(\"d nd st rd th\"))\n",
    "ndot = Literal(\"n.\")\n",
    " \n",
    "# need to forward declare citation, since we will also refer to it\n",
    "# within its own definition (as negative lookahead when parsing\n",
    "# page numbers)\n",
    "citation = Forward()\n",
    " \n",
    "# source_abbrev is very tricky, have to incorporate negative lookaheads\n",
    "# to avoid reading non page numbers as page numbers\n",
    "source_abbrev = originalTextFor(\n",
    "    OneOrMore(ordinal | \n",
    "              (~integer + Word(alphanums+'.')))\n",
    "    )\n",
    " \n",
    "# use the first expression for pages if you want to iterate over each \n",
    "# separate page in the list\n",
    "pages = Group(delimitedList((Group(integer + ndot + integer) | ~citation + integer)))\n",
    "# use the second expression for pages if you just want the list of pages\n",
    "# as a string\n",
    "pages = originalTextFor(delimitedList((Group(integer + ndot + integer) | ~citation + integer)))\n",
    " \n",
    "citation << integer(\"volume\") + source_abbrev(\"abbrev\") + pages(\"pages\")\n",
    " \n",
    "for cit in citation.searchString(raw):\n",
    "    print cit.dump()\n",
    "    # can also reference individual fields as cit.volume, cit.pages, etc.\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123', '-', '45', '-', '6789']\n"
     ]
    }
   ],
   "source": [
    "dash = '-'\n",
    "ssn = Word(nums, exact=3) + dash + Word(nums, exact=2) + dash + Word(nums, exact=4)\n",
    "target = '123-45-6789'\n",
    "result = ssn.parseString(target)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Trondheim is a small city with a university and 140000 inhabitants. Its central bus systems has 42 bus lines, serving 590 stations, with 1900 (departures per) day in average. T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base. The starting point is to automate the function (Garry Weber, 2005) of a route information agent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2005']\n"
     ]
    }
   ],
   "source": [
    "print (re.findall(r\"\\b[^.]+\\([^()]+\\b(\\d{2}|\\d{4})\\s*\\)[^.]*\\.\",text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from juriscraper.lib.html_utils import get_visible_text\n",
    "import reporter_tokenizer\n",
    "\n",
    "FORWARD_SEEK = 20\n",
    "\n",
    "BACKWARD_SEEK = 70 # Average case name length in the db is 67\n",
    "\n",
    "STOP_TOKENS = ['v', 're', 'parte', 'denied', 'citing', \"aff'd\", \"affirmed\",\n",
    "               \"remanded\", \"see\", \"granted\", \"dismissed\"]\n",
    "\n",
    "class Citation(object):\n",
    "    '''Convenience class which represents a single citation found in a document.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, reporter, page, volume):\n",
    "        self.reporter = reporter\n",
    "        self.volume = volume\n",
    "        self.page = page\n",
    "        self.extra = None\n",
    "        self.defendant = None\n",
    "        self.plaintiff = None\n",
    "        self.court = None\n",
    "        self.year = None\n",
    "        self.match_url = None\n",
    "\n",
    "    def base_citation(self):\n",
    "        return u\"%d %s %d\" % (self.volume, self.reporter, self.page)\n",
    "\n",
    "    def as_regex(self):\n",
    "        return r\"%d(\\s+)%s(\\s+)%d\" % (self.volume, self.reporter, self.page)\n",
    "\n",
    "    # TODO: Update css for no-link citations\n",
    "    def as_html(self):\n",
    "        template = u'<span class=\"volume\">%(volume)d</span>\\\\1' \\\n",
    "            u'<span class=\"reporter\">%(reporter)s</span>\\\\2' \\\n",
    "            u'<span class=\"page\">%(page)d</span>'\n",
    "        inner_html = template % self.__dict__\n",
    "        span_class = \"citation\"\n",
    "        if self.match_url:\n",
    "            inner_html = u'<a href=\"%s\">' % self.match_url + inner_html + u'</a>'\n",
    "        else:\n",
    "            span_class += \" no-link\"\n",
    "        return u'<span class=\"%s\">%s</span>' % (span_class, inner_html)\n",
    "\n",
    "    def __repr__(self):\n",
    "        print_string = self.base_citation()\n",
    "        if self.defendant:\n",
    "            print_string = u' '.join([self.defendant, print_string])\n",
    "            if self.plaintiff:\n",
    "                print_string = u' '.join([self.plaintiff, 'v.', print_string])\n",
    "        if self.extra:\n",
    "            print_string = u' '.join([print_string, self.extra])\n",
    "        if self.court and self.year:\n",
    "            paren = u\"(%s %d)\" % (self.court, self.year)\n",
    "        elif self.year:\n",
    "            paren = u'(%d)' % self.year\n",
    "        elif self.court:\n",
    "            paren = u\"(%s)\" % self.court\n",
    "        else:\n",
    "            paren = ''\n",
    "        print_string = u' '.join([print_string, paren])\n",
    "        return print_string.encode(\"utf-8\")\n",
    "\n",
    "# Adapted from nltk Penn Treebank tokenizer\n",
    "def strip_punct(text):\n",
    "    #starting quotes\n",
    "    text = re.sub(r'^\\\"', r'', text)\n",
    "    text = re.sub(r'(``)', r'', text)\n",
    "    text = re.sub(r'([ (\\[{<])\"', r'', text)\n",
    "\n",
    "    #punctuation\n",
    "    text = re.sub(r'\\.\\.\\.', r'', text)\n",
    "    text = re.sub(r'[,;:@#$%&]', r'', text)\n",
    "    text = re.sub(r'([^\\.])(\\.)([\\]\\)}>\"\\']*)\\s*$', r'\\1', text)\n",
    "    text = re.sub(r'[?!]', r'', text)\n",
    "\n",
    "    text = re.sub(r\"([^'])' \", r\"\", text)\n",
    "\n",
    "    #parens, brackets, etc.\n",
    "    text = re.sub(r'[\\]\\[\\(\\)\\{\\}\\<\\>]', r'', text)\n",
    "    text = re.sub(r'--', r'', text)\n",
    "\n",
    "    #ending quotes\n",
    "    text = re.sub(r'\"', \"\", text)\n",
    "    text = re.sub(r'(\\S)(\\'\\')', r'', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def get_court(paren_string, year):\n",
    "    if year is None:\n",
    "        return strip_punct(paren_string)\n",
    "    year_index = paren_string.find(str(year))\n",
    "    return strip_punct(paren_string[:year_index])\n",
    "\n",
    "def get_year(token):\n",
    "    '''Given a string token, look for a valid 4-digit number at the start and \n",
    "    return its value.\n",
    "    '''\n",
    "    token = strip_punct(token)\n",
    "    if not token.isdigit():\n",
    "        # Sometimes funny stuff happens?\n",
    "        token = re.sub(r'(\\d{4}).*', r'\\1', token)\n",
    "        if not token.isdigit():\n",
    "            return None\n",
    "    if len(token) != 4:\n",
    "        return None\n",
    "    year = int(token)\n",
    "    if year < 1754: # Earliest case in the database\n",
    "        return None\n",
    "    return year\n",
    "\n",
    "def add_post_citation(citation, words, reporter_index):\n",
    "    '''Add to a citation object any additional information found after the base\n",
    "    citation, including court, year, and possibly page range.\n",
    "\n",
    "    Examples:\n",
    "        Full citation: 123 U.S. 345 (1894)\n",
    "        Post-citation info: year=1894\n",
    "\n",
    "        Full citation: 123 F.2d 345, 347-348 (4th Cir. 1990)\n",
    "        Post-citation info: year=1990, court=\"4th Cir.\", extra (page range)=\"347-348\"\n",
    "    '''\n",
    "    end_position = reporter_index + 2\n",
    "    # Start looking 2 tokens after the reporter (1 after page)\n",
    "    for start in xrange(reporter_index + 2, min(reporter_index + FORWARD_SEEK, len(words))):\n",
    "        if words[start].startswith('('):\n",
    "            for end in xrange(start, start + FORWARD_SEEK):\n",
    "                if words[end].find(')') > -1:\n",
    "                    # Sometimes the paren gets split from the preceding content\n",
    "                    if words[end].startswith(')'):\n",
    "                        citation.year = get_year(words[end - 1])\n",
    "                    else:\n",
    "                        citation.year = get_year(words[end])\n",
    "                    citation.court = get_court(u' '.join(words[start:end + 1]), citation.year)\n",
    "                    end_position = end\n",
    "                    break\n",
    "            if start > reporter_index + 2:\n",
    "                # Then there's content between page and (), starting with a comma, which we skip\n",
    "                citation.extra = u' '.join(words[reporter_index + 3:start])\n",
    "            break\n",
    "    return end_position\n",
    "\n",
    "def add_defendant(citation, words, reporter_index):\n",
    "    '''Scan backwards from 2 tokens before reporter until you find v., in re, etc.\n",
    "    If no known stop-token is found, no defendant name is stored.  In the future, \n",
    "    this could be improved.'''\n",
    "    start_index = None\n",
    "    for index in xrange(reporter_index - 1, max(reporter_index - BACKWARD_SEEK, 0), -1):\n",
    "        word = words[index]\n",
    "        if word == ',':\n",
    "            # Skip it\n",
    "            continue\n",
    "        if strip_punct(word).lower() in STOP_TOKENS:\n",
    "            if word == 'v.':\n",
    "                citation.plaintiff = words[index - 1]\n",
    "            start_index = index + 1\n",
    "            break\n",
    "        if word.endswith(';'):\n",
    "            # String citation\n",
    "            break\n",
    "    if start_index:\n",
    "        citation.defendant = u' '.join(words[start_index:reporter_index - 1])\n",
    "\n",
    "def extract_base_citation(words, reporter_index):\n",
    "    '''Given a list of words and the index of a federal reporter, look before and after\n",
    "    for volume and page number.  If found, construct and return a Citation object.'''\n",
    "    reporter = words[reporter_index]\n",
    "    # Get rid of extra space so that we only have one version to check\n",
    "    if reporter == 'U. S.':\n",
    "        reporter = 'U.S.'\n",
    "    if words[reporter_index - 1].isdigit():\n",
    "        volume = int(words[reporter_index - 1])\n",
    "    else: # No volume, therefore not a valid citation\n",
    "        return None\n",
    "    page_str = words[reporter_index + 1]\n",
    "    if page_str.find(',') == len(page_str) - 1:\n",
    "        # Strip off ending comma, which occurs when there is a page range next\n",
    "        page_str = page_str[:-1]\n",
    "    if page_str.isdigit():\n",
    "        page = int(page_str)\n",
    "    else: # No page, therefore not a valid citation\n",
    "        return None\n",
    "\n",
    "    return Citation(reporter, page, volume)\n",
    "\n",
    "def get_citations(text, html=True):\n",
    "    if html:\n",
    "        text = get_visible_text(text)\n",
    "    words = reporter_tokenizer.tokenize(text)\n",
    "    citations = []\n",
    "    previous_end_position = 0\n",
    "    # Exclude first and last tokens when looking for reporters, because valid\n",
    "    # citations must have a volume before and a page number after the reporter.\n",
    "    for i in xrange(1,len(words)-1):\n",
    "        # Find reporter\n",
    "        if words[i] in reporter_tokenizer.REPORTERS:\n",
    "            citation = extract_base_citation(words, i)\n",
    "            if citation is None:\n",
    "                # Not a valid citation; continue looking\n",
    "                continue\n",
    "            end_position = add_post_citation(citation, words, i)\n",
    "            add_defendant(citation, words, i)\n",
    "            citations.append(citation)\n",
    "\n",
    "            # Advance the counter; no need to re-check tokens in this citation\n",
    "            i = end_position\n",
    "            previous_end_position = end_position + 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "def getFileContents(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def getCitationsFromFile(filename):\n",
    "    contents = getFileContents(filename)\n",
    "    return get_citations(contents)\n",
    "\n",
    "def getCitationsFromFiles(filenames):\n",
    "    citations = []\n",
    "    for filename in filenames:\n",
    "        citations.extend(getCitationsFromFile(filename))\n",
    "    return citations\n",
    "\n",
    "def main():\n",
    "    citations = []\n",
    "    if len(sys.argv) > 1:\n",
    "        path = sys.argv[1]\n",
    "        filenames = []\n",
    "        for filename in os.listdir(path):\n",
    "            if len(filenames) > 100: break\n",
    "            if not (filename.endswith(\"xml\") or filename.endswith(\"pdf\")):\n",
    "                filenames.append(path + \"/\" + filename)\n",
    "        citations = getCitationsFromFiles(filenames)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
