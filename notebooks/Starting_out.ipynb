{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a library to fetch web content using HTTP requests.\n",
    "import requests\n",
    "\n",
    "# Import a library to parse the HTML content of web pages.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "import nltk   \n",
    "from urllib import urlopen\n",
    "\n",
    "import simplejson\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16-1436', '16-327', '16-309', '15-1191', '16-54', '15-674', '14-1096', '13-1402', '14-185', '13-628']\n"
     ]
    }
   ],
   "source": [
    "## Look at the Data\n",
    "\n",
    "url = \"http://caselaw.findlaw.com\\\n",
    "/summary/search/?query=filters&court=\\\n",
    "us-supreme-court&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1\"\n",
    "\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "\n",
    "# print (soup.prettify())\n",
    "\n",
    "# # print (soup.tbody)\n",
    "\n",
    "# print (soup.find_all('tr'))\n",
    "# print (soup.tr)\n",
    "\n",
    "# print (soup.find_all('valign'=='top'))\n",
    "\n",
    "# containers = soup.findAll('tr',{'class':'srpcaselawtr'})\n",
    "\n",
    "containers = soup.findAll('tr',{'class':'srpcaselawtr'})\n",
    "\n",
    "# print (containers[0].find_all('valign'=='top'))\n",
    "\n",
    "docket_list = []\n",
    "\n",
    "for con in containers: \n",
    "    for item in con.find_all('valign'=='top'): \n",
    "    #     print(ind)\n",
    "    # #     print (type(item))\n",
    "    #     print(item.string)\n",
    "    #     print (len(str(item.string)))\n",
    "\n",
    "        if len(str(item.string)) < 10 and item.string != None and 'v' not in item.string:  \n",
    "            docket_list.append(item.string)\n",
    "        \n",
    "\n",
    "print(docket_list)\n",
    "\n",
    "# print (soup.find_all(tr.td.string))\n",
    "\n",
    "\n",
    "# jumble.find_all('td.string')\n",
    "\n",
    "## print soup.find_all('td')\n",
    "\n",
    "\n",
    "# rows = soup.select('table.mod-data tbody tr')\n",
    "# print soup.find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'16-1436', u'16-327', u'16-309', u'15-1191', u'16-54', u'15-674', u'14-1096', u'13-1402', u'14-185', u'13-628', u'13-1034', u'12-930', u'11-702', u'11-820', u'11-182', u'10-1542', u'10-1211', u'10-699', u'10-577', u'10-694', u'09-115', u'09-60', u'08-1529', u'08-651', u'08-911', u'08-495', u'08-108', u'08-681', u'07-499', u'06-1181', u'06-1717', u'05-1629', u'05-998', u'05-547', u'05-7664', u'04-1376', u'05-552', u'03-878', u'03-674', u'03-583', u'01-1491', u'00-1595', u'99-7791', u'00-767', u'00-1011', u'99-2071']\n"
     ]
    }
   ],
   "source": [
    "### grab ALL docket numbers\n",
    "docket_list = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "\n",
    "    url = \"http://caselaw.findlaw.com/summary/search/?query=filters&court=us-supreme-court&dateFormat=yyyyMMdd&topic=cs_42&pgnum={}\".format(i)\n",
    "\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    containers = soup.findAll('tr',{'class':'srpcaselawtr'})\n",
    "\n",
    "    for con in containers: \n",
    "        for item in con.find_all('valign'=='top'): \n",
    "\n",
    "            if len(str(item.string)) < 10 and item.string != None and 'v' not in item.string:  \n",
    "                docket_list.append(item.string)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "print(docket_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docket_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'http://caselaw.findlaw.com/us-supreme-court/16-1436.html'\n",
    "\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "    script.extract()\n",
    "\n",
    "body = soup.find_all('div',{'class':'caselawContent baseIncluder section'})\n",
    "\n",
    "### Grab only the releant text -- minus the HTML Tags\n",
    "nbody = \"\".join([p.text for p in body])\n",
    "# print(nbody)\n",
    "\n",
    "f = open('../data/src_law/output2.txt', 'w')\n",
    "\n",
    "for item in nbody.encode('utf-8').splitlines(): \n",
    "#     print item + os.linesep\n",
    "    f.write(\"%s\" % item + os.linesep)\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "# print (soup.prettify())\n",
    "\n",
    "# # print (soup.tbody)\n",
    "\n",
    "# print (soup.find_all('tr'))\n",
    "# print (soup.tr)\n",
    "\n",
    "# print (soup.find_all('valign'=='top'))\n",
    "\n",
    "# containers = soup.findAll('tr',{'class':'srpcaselawtr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://caselaw.findlaw.com/us-supreme-court/16-1436.html'\n",
    "# content = requests.get(url).content\n",
    "# soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "    script.extract()\n",
    "\n",
    "raw = soup.get_text()\n",
    "\n",
    "# break into lines and remove leading and trailing space on each\n",
    "lines = (line.strip() for line in raw.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "# print text\n",
    "\n",
    "# itemlist= list(raw)\n",
    "\n",
    "# print(raw)\n",
    "\n",
    "# print text\n",
    "\n",
    "# outfile.write(\"\\n\".join(itemlist))\n",
    "\n",
    "f = open('output1.txt', 'w')\n",
    "\n",
    "for item in text.encode('utf-8').splitlines():\n",
    "#     print item + os.linesep\n",
    "    f.write(\"%s\" % item + os.linesep)\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "# list_of_strip = []\n",
    "# def strip(articles):\n",
    "    \n",
    "#     for article in articles: \n",
    "#         stripped = article.encode('ascii', errors='ignore').strip().translate(None, string.punctuation).decode('ascii')\n",
    "#         stripped = \"\".join(stripped.splitlines())\n",
    "#         list_of_strip.append(stripped.rstrip())\n",
    "        \n",
    "#     return \n",
    "\n",
    "# strip(caption)\n",
    "\n",
    "# list_of_strip[0][0:100]\n",
    "# print text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape up the full text for Supreme Court Cases on Immigration - From 2000 - 2017\n",
    "\n",
    "\n",
    "case_texts= []\n",
    "\n",
    "f = open('../data/src_law/supreme_court.txt', 'w')\n",
    "\n",
    "for docket in docket_list:\n",
    "    url = 'http://caselaw.findlaw.com/us-supreme-court/{}.html'.format(docket)\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "        \n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "\n",
    "    body = soup.find_all('div',{'class':'caselawContent baseIncluder section'})\n",
    "\n",
    "    ### Grab only the releant text -- minus the HTML Tags\n",
    "    nbody = \"\".join([p.text for p in body])\n",
    "\n",
    "\n",
    "    for item in nbody.encode('utf-8').splitlines(): \n",
    "        f.write(\"%s\" % item + os.linesep)\n",
    "\n",
    "\n",
    "#     raw = soup.get_text()\n",
    "#     case_texts.append(raw)\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_texts[-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url = 'http://caselaw.findlaw.com/us-supreme-court/16-1436.html'\n",
    "\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "body = soup.findAll('div',{'class':'caselawContent baseIncluder section'})\n",
    "body\n",
    "\n",
    "# print (soup.prettify())\n",
    "\n",
    "# # print (soup.tbody)\n",
    "\n",
    "# print (soup.find_all('tr'))\n",
    "# print (soup.tr)\n",
    "\n",
    "# print (soup.find_all('valign'=='top'))\n",
    "\n",
    "# containers = soup.findAll('tr',{'class':'srpcaselawtr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case_texts= []\n",
    "\n",
    "f = open('../data/src_law/supreme_court.txt', 'w')\n",
    "\n",
    "for docket in docket_list:\n",
    "    url = 'http://caselaw.findlaw.com/us-supreme-court/{}.html'.format(docket)\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "        \n",
    "    for script in soup([\"script\", \"style\"]): # remove all javascript and stylesheet code\n",
    "        script.extract()\n",
    "\n",
    "    body = soup.find_all('div',{'class':'caselawContent baseIncluder section'})\n",
    "\n",
    "    ### Grab only the releant text -- minus the HTML Tags\n",
    "    nbody = \"\".join([p.text for p in body])\n",
    "\n",
    "\n",
    "    for item in nbody.encode('utf-8').splitlines(): \n",
    "        f.write(\"%s\" % item + os.linesep)\n",
    "\n",
    "\n",
    "#     raw = soup.get_text()\n",
    "#     case_texts.append(raw)\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
