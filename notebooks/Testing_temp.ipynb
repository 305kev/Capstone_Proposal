{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "try:\n",
    "    from urlparse import urljoin\n",
    "except ImportError:\n",
    "    from urllib.parse import urljoin\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_scraper(current_url, dft):\n",
    "    \"\"\"\n",
    "    Run the web scraper that will scrape Findlaw\n",
    "    :param current_url: string, the initial URL to scrape\n",
    "    :param dft: Pandas dataframe, the listings data from previous scrapings\n",
    "    :return: dft: Pandas dataframe containing scraped information\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    soup = create_soup(current_url)\n",
    "    cases = defaultdict(list)\n",
    "\n",
    "    # Run the scraper until it runs out of pages to scrape\n",
    "    flag = u'\\xbb'\n",
    "    last_page = False\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    while u'\\xbb' in flag or last_page==False and count < 3:\n",
    "        # 1.) Get each court case listing\n",
    "        count +=1\n",
    "\n",
    "        for row in soup.find_all(name=\"tr\", attrs={\"class\": \"srpcaselawtr\"}):\n",
    "            try:\n",
    "                cases = add_case_info(row, cases)\n",
    "                sleep(1)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # 2.) Get next page's pagination info + set new flag\n",
    "        try:\n",
    "            current_url = get_next_url(current_url, soup)\n",
    "            soup = create_soup(current_url)\n",
    "            next_list = []\n",
    "            for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}):\n",
    "                next_list.append(i.text)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        flag = next_list[-1]\n",
    "        if u'\\xbb' not in flag and last_page == False:\n",
    "            flag = u'\\xbb'\n",
    "            last_page = True\n",
    "        \n",
    "    dft = dft.append(pd.DataFrame(cases), ignore_index=True)\n",
    "    return dft\n",
    "\n",
    "def create_soup(url):\n",
    "    \"\"\"\n",
    "    Get the HTML contents of the URL.\n",
    "    :param url: string, the url to scrape\n",
    "    :return: soup: a BeautifulSoup object\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    return BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "def get_next_url(current_url, soup):\n",
    "    \"\"\"\n",
    "    Get the URL of the next listings page.\n",
    "    :param soup: Beautiful soup object\n",
    "    :return: string, the URL of the next webpage\n",
    "    \"\"\"\n",
    "\n",
    "    d = [i.get('href') for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"})][-1]\n",
    "    return urljoin(current_url, d)\n",
    "\n",
    "def add_case_info(row, case_dict):\n",
    "    \"\"\"\n",
    "    Get the results of scraping a single case.\n",
    "    :param row: the contents of a single court case's row tag\n",
    "    :param case_dict: the currently scraped court cases\n",
    "    :return: case_dict, the desired scraping information\n",
    "\n",
    "    \"\"\"\n",
    "    case_dict[\"date\"] += [get_date(row)]\n",
    "    case_dict[\"court\"] += [get_court(row)]\n",
    "    case_dict[\"case_title\"] += [get_case_title(row)]\n",
    "    case_dict[\"docket\"] += [get_docket(row)]\n",
    "    case_dict[\"tags\"] += [get_tags(row)]\n",
    "    case_dict[\"type_of_law\"] += [\"case\"]\n",
    "    case_dict[\"web_source\"] += [\"findlaw.com\"]\n",
    "\n",
    "    case_link = get_case_url(row)\n",
    "\n",
    "    case_dict[\"url\"] += [case_link]\n",
    "    case_dict[\"case_text\"] += [get_case_text(case_link)]\n",
    "\n",
    "    return case_dict\n",
    "\n",
    "def get_case_title(row):\n",
    "    \"\"\"\n",
    "    extract the title from the row tag\n",
    "    :row: tag object, the row tag from a case\n",
    "    :return: string, the case title\n",
    "    \"\"\"\n",
    "    title = row.find('a').get(\"title\")\n",
    "    return title\n",
    "\n",
    "def get_court(row):\n",
    "    \"\"\"\n",
    "    extract the court name from the row tag\n",
    "    :row: tag object, the row tag from a case\n",
    "    :return: string, the court\n",
    "    \"\"\"\n",
    "    court = row.find('span').text\n",
    "    return court\n",
    "\n",
    "def get_tags(row):\n",
    "    \"\"\"\n",
    "    extract the types of law involved as law tags from the row tag\n",
    "    :row: tag object, the row tag from a case\n",
    "    :return: string, the law tags\n",
    "    \"\"\"\n",
    "    tags = row.find('i').text\n",
    "    return tags\n",
    "\n",
    "def get_date(row):\n",
    "    \"\"\"\n",
    "    extract decision date from the row tag\n",
    "    :row: tag object, the row tag from a case\n",
    "    :return: string, decision date\n",
    "    \"\"\"\n",
    "    decision_date = row.find_all('td', {'valign': 'top'})[-2].text\n",
    "    return decision_date\n",
    "\n",
    "def get_docket(row):\n",
    "    \"\"\"\n",
    "    extract docket number from the row tag\n",
    "    :row: tag object, the row tag from a case\n",
    "    :return: string, docket number\n",
    "    \"\"\"\n",
    "    docket_number = row.find_all('td', {'valign': 'top'})[-1].text\n",
    "    return docket_number\n",
    "\n",
    "def get_case_url(row):\n",
    "    \"\"\"\n",
    "    extract case URL from the row tag\n",
    "    :row: tag object, the row tag from a case\n",
    "    :return: string, the URL of case text\n",
    "    \"\"\"\n",
    "    initial_url = row.find('a').get('href')\n",
    "    new_soup = create_soup(initial_url)\n",
    "    check_url = new_soup.find(name=\"div\", attrs={\"class\": \"btn_read\"}).find('a').get('href')\n",
    "    check_soup = create_soup(check_url)\n",
    "\n",
    "    if check_soup.find_all(\"a\", {\"target\": \"new\"}):\n",
    "        case_url = second_check(check_url)\n",
    "    else:\n",
    "        case_url = check_url\n",
    "\n",
    "    return case_url\n",
    "\n",
    "def second_check(url):\n",
    "    soup = create_soup(url)\n",
    "    for i in soup.find_all(\"p\"):\n",
    "        try:\n",
    "            href = i.a.get('href')\n",
    "            return href\n",
    "            break\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def get_case_text(link):\n",
    "    \"\"\"\n",
    "    Get the raw text of the case from the linked webpage.\n",
    "    :param link: str, the url of case text webpage\n",
    "    :return: if case text as html: str, the case text from the webpage\n",
    "            else: nothing.\n",
    "    \"\"\"\n",
    "    text_soup = create_soup(link)\n",
    "\n",
    "    # remove all javascript and stylesheet code\n",
    "    for script in text_soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "\n",
    "    text = text_soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\n",
    "    # drop any blank lines and get only relevant text\n",
    "    case = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if chunk and 'United States Court of Appeals' in chunk:\n",
    "            case.append(chunk)\n",
    "            for chunk in chunks:\n",
    "                if 'FindLaw Career Center' in chunk:\n",
    "                    break\n",
    "                else:\n",
    "                    case.append(chunk)\n",
    "    return '\\n'.join(case)\n",
    "\n",
    "def create_df_new():\n",
    "    \"\"\"\n",
    "    If it doesn't exist, create the initial case_data file\n",
    "    :return: New Dataframe\n",
    "    \"\"\"\n",
    "    df_new = pd.DataFrame(columns=[\"date\", \"court\", \"case_title\",\n",
    "                                   \"docket\", \"tags\", \"type_of_law\",\n",
    "                                   \"web_source\", \"url\", \"case_text\"])\n",
    "    return df_new\n",
    "\n",
    "def write_file_to_s3(df_write, court):\n",
    "    \"\"\"\n",
    "    Save the updated dataframe to a file on the project's AWS S3 bucket.\n",
    "    :param df_write: DataFrame to write to file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for column in df:\n",
    "        df_write[column] = df_write[column].str.encode('utf-8')\n",
    "\n",
    "    try:\n",
    "        csv_buffer = StringIO()\n",
    "        df_write.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    except:\n",
    "        csv_buffer = BytesIO()\n",
    "        df_write.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    s3 = boto3.resource(\"s3\", aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "    s3.Object(\"court-case-data\", \"{}.csv\".format(court)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "def access_s3_to_df(court):\n",
    "    \"\"\"\n",
    "    Access the project's S3 bucket and load the file into a dataframe for appending.\n",
    "    :return: df: a pandas dataframe containing the data.\n",
    "    \"\"\"\n",
    "\n",
    "    s3 = boto3.client(\"s3\", aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                      aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=\"court-case-data\", Key=\"{}.csv\".format(court))\n",
    "        return pd.read_csv(BytesIO(obj[\"Body\"].read()))\n",
    "    except:\n",
    "        return create_df_new()\n",
    "\n",
    "def which_court_url(court):\n",
    "    \"\"\"\n",
    "    :param court: command line argument,\n",
    "    :return: URL for first page of search\n",
    "    \"\"\"\n",
    "    first_url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-{}-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'.format(court)\n",
    "    return first_url\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which court would you like to scrape? (e.g. 1st, 2nd, 3rd, etc.): 10th\n",
      "Which court would you like to scrape? (e.g. 1st, 2nd, 3rd, etc.): 10th\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4b472412bc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccess_s3_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfirst_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhich_court_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_scraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# write_file_to_s3(df, court)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-86e02cf10e95>\u001b[0m in \u001b[0;36mrun_scraper\u001b[0;34m(current_url, dft)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mlast_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mdft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinmagana/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    274\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinmagana/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinmagana/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   5592\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5594\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5595\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5596\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kevinmagana/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   5640\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5641\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5642\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    court= input('Which court would you like to scrape? (e.g. 1st, 2nd, 3rd, etc.): ')\n",
    "except SyntaxError:\n",
    "    court = raw_input('Which court would you like to scrape? (e.g. 1st, 2nd, 3rd, etc.): ')\n",
    "\n",
    "df = access_s3_to_df(court)\n",
    "first_url = which_court_url(court)\n",
    "df= run_scraper(first_url, df)\n",
    "\n",
    "# write_file_to_s3(df, court)\n",
    "print(df.tail())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
