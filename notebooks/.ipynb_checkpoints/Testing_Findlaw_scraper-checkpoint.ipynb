{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "import re\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Look at the Data\n",
    "\n",
    "# print soup.prettify() \n",
    "\n",
    "# print soup.title\n",
    "\n",
    "# print soup.table.find_all('td')\n",
    "\n",
    "# print soup.td.string\n",
    "\n",
    "# jumble.find_all('td.string')\n",
    "\n",
    "## print soup.find_all('td')\n",
    "\n",
    "\n",
    "# rows = soup.select('table.mod-data tbody tr')\n",
    "# print soup.find_all('td')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/12/04/281064.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/10/27/280799.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/09/08/280418.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/08/25/280319.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/08/16/280174.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/07/26/279970.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/07/24/279960.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/06/23/279745.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/06/08/279591.html\n",
      "http://caselaw.findlaw.com/summary/opinion/us-1st-circuit/2017/05/26/279495.html\n"
     ]
    }
   ],
   "source": [
    "url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-1st-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'\n",
    "\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "\n",
    "# print soup.prettify() \n",
    "\n",
    "\n",
    "# for i in soup.find_all(name=\"span\"): \n",
    "#     print i\n",
    "\n",
    "url_list = []\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    if 'circuit' in link.get('href'): \n",
    "        url_list.append(link.get('href'))\n",
    "\n",
    "for i in (url_list): \n",
    "    print i\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Check Function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEEDED A SECOND CHECK\n",
      "BUT ADDED:  http://media.ca1.uscourts.gov/pdf.opinions/16-2152P-01A.pdf On the Second Check\n",
      "ON THE FIRST TRY, ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1878450.html\n",
      "NEEDED A SECOND CHECK\n",
      "BUT ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1873444.html On the Second Check\n",
      "NEEDED A SECOND CHECK\n",
      "BUT ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1872113.html On the Second Check\n",
      "ON THE FIRST TRY, ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1871116.html\n",
      "NEEDED A SECOND CHECK\n",
      "BUT ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1868917.html On the Second Check\n",
      "NEEDED A SECOND CHECK\n",
      "BUT ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1868683.html On the Second Check\n",
      "ON THE FIRST TRY, ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1865790.html\n",
      "ON THE FIRST TRY, ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1863578.html\n",
      "ON THE FIRST TRY, ADDED:  http://caselaw.findlaw.com/us-1st-circuit/1862240.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'http://media.ca1.uscourts.gov/pdf.opinions/16-2152P-01A.pdf',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1878450.html',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1873444.html',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1872113.html',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1871116.html',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1868917.html',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1868683.html',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1865790.html',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1863578.html',\n",
       " u'http://caselaw.findlaw.com/us-1st-circuit/1862240.html']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Function for checkin every site \n",
    "\n",
    "\n",
    "def second_check(url):\n",
    "    \n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    for i in soup.find_all(\"p\"): \n",
    "#         print i, type(i)\n",
    "        try: \n",
    "            href= i.a.get('href')\n",
    "            return  href\n",
    "            break\n",
    "    \n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "def first_check(url_list): \n",
    "    \n",
    "    target_urls = []\n",
    "\n",
    "    for site in url_list:\n",
    "        new_soup = create_soup(site)\n",
    "        \n",
    "        for html in new_soup.find_all(name=\"div\", attrs={\"class\": \"btn_read\"}): \n",
    "            check_url= html.a.get('href')\n",
    "            check_soup = create_soup(check_url)\n",
    "            \n",
    "            if check_soup.find_all(\"a\", { \"target\" : \"new\"}): \n",
    "                print \"NEEDED A SECOND CHECK\"\n",
    "                target_urls.append(second_check(check_url))\n",
    "                print \"BUT ADDED: \", second_check(check_url), \"On the Second Check\"\n",
    "\n",
    "            else: \n",
    "                target_urls.append(check_url)\n",
    "                print 'ON THE FIRST TRY, ADDED: ', check_url\n",
    "\n",
    "        sleep(2)\n",
    "    \n",
    "    return target_urls\n",
    "\n",
    "first_check(url_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://media.ca1.uscourts.gov/pdf.opinions/16-2152P-01A.pdf\n"
     ]
    }
   ],
   "source": [
    "url_not= 'http://caselaw.findlaw.com/us-1st-circuit/1881679.html'\n",
    "\n",
    "def second_check(url):\n",
    "    \n",
    "    content = requests.get(url_not).content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "#     soup = create_soup(site)\n",
    "    \n",
    "    answer = []\n",
    "    for i in soup.find_all(\"p\"): \n",
    "#         print i, type(i)\n",
    "        try: \n",
    "            href= i.a.get('href')\n",
    "            print href\n",
    "#             answer.apend(href)\n",
    "#             print answer\n",
    "            break\n",
    "    \n",
    "        except:\n",
    "            continue \n",
    "\n",
    "second_check(url_not)\n",
    "\n",
    "# print soup.find_all(\"div\", {\"p\" : \"caselawcontent searchable-content\"})\n",
    "\n",
    "\n",
    "\n",
    "# content = requests.get(url_not).content\n",
    "# soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "# for i in soup.find_all(\"p\"): \n",
    "# #     print i, type(i)\n",
    "    \n",
    "#     try: \n",
    "#         href= i.a.get('href') \n",
    "#         print href\n",
    "#         break\n",
    "        \n",
    "#     except:\n",
    "#         print \"didnt work!!\"\n",
    "#         continue \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Check Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #.get('href')\n",
    "#     print i.get('href')\n",
    "#     print link.get('href')\n",
    "#     if 'circuit' in link.get('href'): \n",
    "#         print link.get('href')\n",
    "\n",
    "    \n",
    "#     for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "#         next_list.append(i.text)\n",
    "\n",
    "# def second_check(url): \n",
    "    \n",
    "    \n",
    "    \n",
    "#     return url \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_scraper(current_url, dft):\n",
    "    \"\"\"\n",
    "    Run the web scraper that will scrape Findlaw\n",
    "    :param current_url: string, the initial URL to scrape\n",
    "    :param dft: Pandas dataframe, the listings data from previous scrapings\n",
    "    :return: dft: Pandas dataframe containing scraped information\n",
    "    \"\"\"\n",
    "#     content = requests.get(url).content\n",
    "#     soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "\n",
    "#     for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "#         next_list.append(i.text)\n",
    "    \n",
    "    flag = \"\\xbb\"\n",
    "    list_of_urls = []\n",
    "\n",
    "    # Run the scraper until it runs out of pages to scrape\n",
    "\n",
    "    while \"\\xbb\" in flag:\n",
    "        soup = create_soup(current_url)\n",
    "        \n",
    "        next_list =[]\n",
    "        \n",
    "        for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "            next_list.append(i.text)\n",
    "    \n",
    "        flag = next_list[-1]\n",
    "\n",
    "        \n",
    "        for div in my_soup.find_all(name=\"div\", attrs={\"class\": \"row\"}):\n",
    "            listings = add_listing_info(div, listings)\n",
    "            sleep(2)\n",
    "            \n",
    "        current_url = get_next_url(my_soup)\n",
    "        sleep(2)\n",
    "    dft = dft.append(pd.DataFrame(listings), ignore_index=True)\n",
    "    return dft\n",
    "\n",
    "    while type(int(next_list[-1])) != type(int): \n",
    "        \n",
    "\n",
    "current_url = url\n",
    "run_scraper(current_url, dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1', u'2', u'3', u'4', u'5', u'\\xbb']\n"
     ]
    }
   ],
   "source": [
    "next_list =[]\n",
    "\n",
    "for i in soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"}): \n",
    "    next_list.append(i.text)\n",
    "    \n",
    "print next_list\n",
    "\n",
    "# next_list= next_list[0:-1]\n",
    "\n",
    "\n",
    "\n",
    "# try: \n",
    "#     type(int(next_list[-1])) != type(int)\n",
    "    \n",
    "#     print \"Yes!!\"\n",
    "    \n",
    "# except: \n",
    "#     print \"No!!\"\n",
    "    \n",
    "\n",
    "# print next_list\n",
    "\n",
    "# while type(int(next_list[-1])) != type(int): \n",
    "#     for i in (range(1,5)):\n",
    "#         next_list.append(i)\n",
    "    \n",
    "#     print next_list\n",
    "#     next_list.append('i')\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "# print soup.find_all(name=\"a\", attrs={\"class\": \"pgnum\"})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print soup.find(name=\"span\", attrs={\"class\": \"pgnum\"}).text\n",
    "                \n",
    "# for next_ in soup.find_all(name=\"span\", attrs={\"a\": \"title\"}): \n",
    "#     print next_['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for next_ in soup.find_all(name=\"span\", attrs={\"a\": \"title\"}): \n",
    "    print next_['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://caselaw.findlaw.com/summary/search/?query=filters&court=us-1st-circuit&dateFormat=yyyyMMdd&topic=cs_42&pgnum=1'\n",
    "\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "next_list =[]\n",
    "  \n",
    "while type(int(next_list[-1])) != type(int): \n",
    "    for i in (range(1,5)):\n",
    "        next_list.append(i)\n",
    "    \n",
    "    next_list.append('i')\n",
    "    \n",
    "print next_list\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
